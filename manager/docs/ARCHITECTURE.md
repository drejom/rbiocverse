# rbiocverse Manager - Architecture

## Overview

The rbiocverse Manager is a web application that provides browser-based IDE experiences (VS Code, RStudio, JupyterLab) on HPC SLURM clusters. It manages job submission, SSH tunneling, and proxies requests to IDEs running in Singularity containers on compute nodes.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         User Browser                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Launcher   â”‚  â”‚     IDE (iframe) - VS Code / RStudio / Jupyterâ”‚  â”‚
â”‚  â”‚    (React)  â”‚  â”‚  /code/ â”‚ /rstudio/ â”‚ /jupyter/ â†’ proxy      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚ HTTPS
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    rbiocverse Manager                                â”‚
â”‚                      (Express.js on Dokploy)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  server.ts - Main orchestration                               â”‚   â”‚
â”‚  â”‚    â”œâ”€â”€ Static files (public/, ui/dist/)                       â”‚   â”‚
â”‚  â”‚    â”œâ”€â”€ API routes (/api/*)                                    â”‚   â”‚
â”‚  â”‚    â””â”€â”€ HTTP Proxy (VS Code :8000, RStudio :8787, Jupyter :8888â”‚   â”‚
â”‚  â”‚                     hpc-proxy :9000)                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ HpcService â”‚ â”‚TunnelServiceâ”‚ â”‚StateManagerâ”‚ â”‚   Validation   â”‚   â”‚
â”‚  â”‚(SLURM ops) â”‚ â”‚ (SSH tunnels)â”‚ â”‚(sub-modules)â”‚ â”‚   (security)   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚ SSH Tunnel (IDE port via hpc-proxy)
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    HPC Login Node                                    â”‚
â”‚                 (gemini-login2 / ppxhpcacc01)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  SSH tunnel: localhost:9000 â†’ compute-node:<hpc-proxy-port>  â”‚   â”‚
â”‚  â”‚  hpc-proxy routes /port/:port/* to localhost:<port> on node  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SLURM Compute Node                                â”‚
â”‚                     (e.g., g-h-1-9-25)                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Singularity Container (rbiocverse_X.XX.sif)                 â”‚   â”‚
â”‚  â”‚    â”œâ”€â”€ VS Code: code serve-web on dynamic port (~8000)       â”‚   â”‚
â”‚  â”‚    â”œâ”€â”€ RStudio: rserver on dynamic port (~8787)              â”‚   â”‚
â”‚  â”‚    â”œâ”€â”€ JupyterLab: jupyter lab on dynamic port (~8888)       â”‚   â”‚
â”‚  â”‚    â””â”€â”€ hpc-proxy: Go binary routing /port/:port/* requests   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Supported IDEs

| IDE | Default Port | Proxy Path | Bioc 3.22 | Bioc 3.19 | Bioc 3.18 | Bioc 3.17 |
|-----|-------------|------------|-----------|-----------|-----------|-----------|
| VS Code | 8000 | `/code/` | âœ“ | âœ“ | âœ“ | âœ“ |
| RStudio | 8787 | `/rstudio/` | âœ“ | âœ“ | âœ“ | âœ“ |
| JupyterLab | 8888 | `/jupyter/` | âœ“ | â€” | â€” | â€” |

Ports are dynamically discovered â€” the job script finds a free port and writes it to `~/.<ide>-slurm/port`, which the manager reads via SSH after node assignment.

## Directory Structure

```
manager/
â”œâ”€â”€ server.ts              # Main Express server + IDE proxies
â”œâ”€â”€ config/
â”‚   â””â”€â”€ index.ts           # Cluster, IDE, GPU and release configuration
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ validation.ts      # Security-critical input validation
â”‚   â”œâ”€â”€ helpers.ts         # Time parsing and formatting utilities
â”‚   â”œâ”€â”€ state.ts           # StateManager orchestrator (thin, delegates to sub-modules)
â”‚   â”œâ”€â”€ state/
â”‚   â”‚   â”œâ”€â”€ types.ts       # Shared types, constants, and utility functions
â”‚   â”‚   â”œâ”€â”€ index.ts       # Barrel re-export
â”‚   â”‚   â”œâ”€â”€ locking.ts     # LockManager - operation mutex
â”‚   â”‚   â”œâ”€â”€ sessions.ts    # SessionManager - in-memory session CRUD
â”‚   â”‚   â”œâ”€â”€ jobPolling.ts  # JobPoller - adaptive SLURM job polling loop
â”‚   â”‚   â””â”€â”€ clusterHealth.ts # ClusterHealthPoller - fixed-interval health polling
â”‚   â”œâ”€â”€ db.ts              # SQLite initialization and connection
â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â”œâ”€â”€ users.ts       # User account CRUD
â”‚   â”‚   â”œâ”€â”€ sessions.ts    # Active session write-through and archiving
â”‚   â”‚   â”œâ”€â”€ health.ts      # Cluster health snapshots and cache
â”‚   â”‚   â”œâ”€â”€ analytics.ts   # Analytics queries (usage, growth, queue wait)
â”‚   â”‚   â”œâ”€â”€ partitions.ts  # Partition limits CRUD
â”‚   â”‚   â””â”€â”€ migrate.ts     # JSON â†’ SQLite migration
â”‚   â”œâ”€â”€ errors.ts          # Custom error classes and helpers
â”‚   â”œâ”€â”€ logger.ts          # Winston structured logging with domain prefixes
â”‚   â””â”€â”€ asyncHandler.ts    # Express async route error wrapper
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ hpc.ts             # HpcService - SLURM operations via SSH
â”‚   â”œâ”€â”€ tunnel.ts          # TunnelService - SSH tunnel management
â”‚   â””â”€â”€ notifications.ts   # Email/notification service
â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ index.ts       # Combined API router factory
â”‚   â”‚   â”œâ”€â”€ helpers.ts     # Shared helpers, interfaces, singletons
â”‚   â”‚   â”œâ”€â”€ status.ts      # /health, /status, /cluster-status, /dev-servers
â”‚   â”‚   â”œâ”€â”€ sessions.ts    # /launch, /switch, /stop, /stop-all
â”‚   â”‚   â””â”€â”€ streaming.ts   # SSE: /launch/:hpc/:ide/stream, /stop/:hpc/:ide/stream
â”‚   â”œâ”€â”€ auth.ts            # Authentication endpoints (/api/auth/*)
â”‚   â”œâ”€â”€ help.ts            # Help content with template processing (/api/help/*)
â”‚   â”œâ”€â”€ admin.ts           # Admin dashboard endpoints (/api/admin/*)
â”‚   â”œâ”€â”€ stats.ts           # Public stats endpoints (/api/stats/*)
â”‚   â””â”€â”€ clientErrors.ts    # Frontend error reporting (/api/client-errors)
â”œâ”€â”€ content/
â”‚   â”œâ”€â”€ help/              # Markdown help files + index.json
â”‚   â””â”€â”€ admin/             # Markdown admin docs + index.json
â”œâ”€â”€ ui/                    # React 19 frontend (Vite 7)
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ App.tsx        # Root component with provider hierarchy
â”‚       â”œâ”€â”€ components/    # React components
â”‚       â”‚   â”œâ”€â”€ ContentPanel.tsx   # Shared slide-out panel (Help + Admin)
â”‚       â”‚   â”œâ”€â”€ HelpPanel.tsx      # Help slide-out (uses ContentPanel)
â”‚       â”‚   â”œâ”€â”€ AdminPanel.tsx     # Admin slide-out (uses ContentPanel)
â”‚       â”‚   â”œâ”€â”€ help-widgets/      # Embeddable help widgets
â”‚       â”‚   â””â”€â”€ admin-widgets/     # Admin analytics widgets (D3.js)
â”‚       â”œâ”€â”€ contexts/      # React contexts (Auth, SessionState, Theme)
â”‚       â”œâ”€â”€ hooks/         # Custom hooks (useApi, useClusterStatus, useLaunch)
â”‚       â””â”€â”€ types/         # TypeScript type definitions
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ hpc-proxy/         # Go binary: /port/:port/* reverse proxy (in container)
â”œâ”€â”€ public/                # Static assets, wrapper pages (menu-frame.html)
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ unit/              # Unit tests (mocha + chai)
â”‚   â””â”€â”€ integration/       # API integration tests
â””â”€â”€ docs/                  # Documentation
```

## Key Components

### 1. StateManager (`lib/state.ts` + `lib/state/`)

The StateManager is a thin orchestrator (~100 lines) that delegates to four focused sub-managers sharing a common `AppState` object by reference:

| Sub-manager | File | Responsibility |
|---|---|---|
| `LockManager` | `lib/state/locking.ts` | Mutex per operation name; prevents concurrent launches |
| `SessionManager` | `lib/state/sessions.ts` | In-memory session CRUD, SQLite write-through |
| `JobPoller` | `lib/state/jobPolling.ts` | Adaptive SLURM polling (15sâ€“30min backoff) |
| `ClusterHealthPoller` | `lib/state/clusterHealth.ts` | Fixed 30-min health polling + history |

```typescript
// State structure - sessions keyed by "${user}-${hpc}-${ide}"
{
  sessions: {
    'domeally-gemini-vscode': {
      status: 'running',        // idle | starting | pending | running
      jobId: '28692461',
      node: 'g-h-1-9-25',
      ide: 'vscode',
      releaseVersion: '3.22',
      gpu: 'a100',              // null for CPU-only
      cpus: 4,
      memory: '40G',
      walltime: '12:00:00',
      startedAt: '2025-01-01T10:00:00.000Z',
      submittedAt: '2025-01-01T09:59:50.000Z',
    },
  },
  activeSession: { user: 'domeally', hpc: 'gemini', ide: 'vscode' } | null,
  clusterHealth: { gemini: { current: {...}, history: [...] }, apollo: {...} }
}

// Lifecycle
await stateManager.load();           // Load from SQLite, reconcile with SLURM
stateManager.startPolling(factory);  // Start background polling loops
stateManager.stopPolling();          // Stop polling (graceful shutdown)

// Operation locks (prevent concurrent launches per user/cluster/ide)
stateManager.acquireLock('launch:domeally-gemini-vscode');  // Throws if locked
stateManager.releaseLock('launch:domeally-gemini-vscode');
```

**Adaptive polling intervals** (based on session activity):
- 15 seconds: any pending/near-expiry session
- 1 minute: running session
- 5â€“30 minutes: exponential backoff when stable (no state changes)

### 2. HpcService (`services/hpc.ts`)

Handles SLURM operations via SSH with ControlMaster multiplexing. All SSH commands are serialized per cluster via `withClusterQueue` to prevent connection flooding.

```typescript
const hpc = new HpcService('gemini', 'domeally');

// Submit job with IDE and release-specific settings
const { jobId, token } = await hpc.submitJob(4, '40G', '12:00:00', 'vscode', {
  releaseVersion: '3.22',
  gpu: 'a100',  // or '' for CPU
});

// Get current job info (from squeue, pipe-delimited)
const info = await hpc.getJobInfo('vscode');
// { jobId, state, node, timeLeft, timeLimit, cpus, memory, startTime }

// Get all IDE jobs for user in one SSH call
const jobs = await hpc.getAllJobs();
// { vscode: JobInfo|null, rstudio: JobInfo|null, jupyter: JobInfo|null }

// Read dynamic port written by job script
const port = await hpc.getIdePort('vscode');  // reads ~/.vscode-slurm/port
const proxyPort = await hpc.getProxyPort('domeally');  // reads ~/.hpc-proxy/port

// Cancel job(s)
await hpc.cancelJob(jobId);
await hpc.cancelJobs([jobId1, jobId2]);  // batch scancel
```

**Parallel processing env vars** set automatically in all job scripts:
- `OMP_NUM_THREADS`, `MKL_NUM_THREADS`, `OPENBLAS_NUM_THREADS` (linear algebra)
- `NUMEXPR_NUM_THREADS` (NumPy), `MC_CORES` (R parallel), `BIOCPARALLEL_WORKER_NUMBER`

**SSH key management:** Per-user private keys loaded from in-memory session store, written to `data/ssh-keys/<username>.key` with 600 permissions. ControlMaster sockets at `/tmp/rbiocverse-ssh/<user>-<cluster>`.

### 3. TunnelService (`services/tunnel.ts`)

Manages SSH tunnels to compute nodes. Session key format: `<user>-<hpc>-<ide>`.

```typescript
// Start tunnel with dynamic port discovery
const tunnelProcess = await tunnelService.start('gemini', 'g-h-1-9-25', 'vscode', onExit, {
  remotePort: 8012,   // dynamically discovered port on compute node
  user: 'domeally',
  proxyPort: 43721,   // hpc-proxy port (VS Code only)
});

// Stop/check tunnel
tunnelService.stop('gemini', 'vscode', 'domeally');
tunnelService.isActive('gemini', 'vscode', 'domeally');
```

**VS Code tunneling** (via hpc-proxy): Maps `localhost:9000 â†’ node:<hpc-proxy-port>`. The hpc-proxy Go binary inside the container routes `/port/<targetPort>/*` to any dev server (Live Server, Shiny, etc.) without individual per-port tunnels.

**Legacy tunneling** (RStudio, JupyterLab): Direct `-L <localPort>:node:<remotePort>` SSH forwarding with `ServerAliveInterval=30`, `ExitOnForwardFailure=yes`.

### 4. hpc-proxy (`tools/hpc-proxy/`)

A Go binary (`main.go` + `proxy.go`) running inside the Singularity container on the compute node.

**Routes:** `GET/POST/WebSocket /port/<targetPort>/...` â€” strips the prefix, proxies to `127.0.0.1:<targetPort>`.

**HTML rewriting** (`--base-rewrite` flag): Injects `<base>` tags, rewrites `href`/`src` absolute paths, and fixes `Location` redirect headers so apps served at `/port/5500/` resolve relative URLs correctly.

The VS Code job script launches hpc-proxy at startup and writes its port to `~/.hpc-proxy/port`. After node assignment, the manager reads this port via SSH and establishes a single tunnel for all dev servers.

### 5. Validation (`lib/validation.ts`)

Security-critical input validation to prevent command injection in SLURM parameters.

```typescript
// Validates CPUs (1-128), memory (e.g., "40G"), time ("HH:MM:SS" or "D-HH:MM:SS")
// Also checks against per-partition limits when hpc and gpu are provided
validateSbatchInputs(cpus, mem, time, hpc, gpu);

// Validates cluster name against whitelist
validateHpcName('gemini');  // OK
validateHpcName('invalid'); // Throws
```

## API Endpoints

### Authentication (`routes/auth.ts`)

| Endpoint | Purpose |
|---|---|
| `POST /api/auth/login` | Verify credentials, issue JWT, test SSH, decrypt key |
| `POST /api/auth/logout` | Invalidate server-side session |
| `GET /api/auth/session` | Check validity, return user info (sliding token refresh) |
| `POST /api/auth/complete-setup` | Mark SSH setup complete |
| `POST /api/auth/test-connection/:cluster` | Test SSH connectivity |
| `POST /api/auth/test-connection-both` | Test both clusters in parallel |
| `POST /api/auth/generate-key` | Generate Ed25519 managed key |
| `POST /api/auth/regenerate-key` | Replace existing managed key |
| `POST /api/auth/remove-key` | Remove managed key (SSH must be working) |
| `GET /api/auth/public-key` | Return user's public key |
| `POST /api/auth/import-key` | Import existing private key (tests SSH, encrypts) |

### Session Management (`routes/api/`)

| Endpoint | Purpose |
|---|---|
| `GET /api/health` | Health check (503 during startup) |
| `GET /api/status` | Instant session state from cache (no SSH) |
| `GET /api/cluster-status` | SLURM + health data; cached 30 min, invalidated on user action |
| `GET /api/dev-servers` | Check active dev server ports (5500, 3838) on compute node |
| `GET /api/launch/:hpc/:ide/stream` | **SSE** - launch with real-time progress events |
| `GET /api/stop/:hpc/:ide/stream` | **SSE** - stop with indeterminate progress |
| `POST /api/launch` | Launch (blocking, no SSE - legacy) |
| `POST /api/switch/:hpc/:ide` | Switch active session to different cluster/IDE |
| `POST /api/stop/:hpc/:ide` | Stop session (optionally cancel SLURM job) |
| `POST /api/stop-all/:hpc` | Batch cancel all user's jobs on a cluster |

### Help / Admin / Stats

| Route | Endpoints |
|---|---|
| `GET /api/help/*` | Help content with live `{{template}}` interpolation |
| `GET /api/admin/*` | Admin dashboard, user management, analytics (auth required) |
| `GET /api/stats/*` | Public anonymized stats (no auth) |
| `POST /api/client-errors` | Frontend error reporting |

## Data Flow

### Launch Session Flow (SSE Streaming)

```
1. User selects cluster/IDE/release/GPU/resources in React UI
2. GET /api/launch/gemini/vscode/stream?releaseVersion=3.22&gpu=a100&cpus=4&mem=40G
3. SSE connection opened; LoadingOverlay shown in UI
4. API acquires lock ('launch:domeally-gemini-vscode')
5. HpcService.submitJob() â†’ SSH sbatch â†’ jobId returned
6. HpcService.checkJobStatus() â†’ quick squeue poll (2 attempts, ~5s)
   - If RUNNING: continue to step 7
   - If PENDING: send { type: 'pending', startTime } â†’ overlay closes, pending card shown
7. HpcService.getIdePort() + getProxyPort() â†’ read dynamic ports via SSH
8. TunnelService.start() â†’ SSH -L 9000:node:<proxyPort>; wait for IDE HTTP ready
9. StateManager.updateSession() â†’ { status: 'running', jobId, node, ... }
10. Lock released; SSE { type: 'complete', redirectUrl: '/code/' } â†’ browser navigates
```

### Proxy Flow

```
1. User loads /code/ (or /rstudio/ or /jupyter/)
2. Express serves ide-wrapper.html (iframe)
3. iframe src="/vscode-direct/" (or /rstudio-direct/, /jupyter-direct/)
4. http-proxy forwards to localhost:8000 (or 8787, 8888)
5. SSH tunnel forwards to compute-node:<hpc-proxy-port> (VS Code)
   or directly to compute-node:<remotePort> (RStudio, JupyterLab)
6. IDE server responds
```

### Dev Server Proxy Flow (VS Code only)

```
1. User opens Shiny app or Live Server preview at /port/5500/
2. Express portProxy â†’ localhost:9000 (hpc-proxy tunnel)
3. hpc-proxy on compute node routes /port/5500/* â†’ localhost:5500
4. Dev server responds; hpc-proxy optionally rewrites HTML base tags
```

## Configuration

### Environment Variables

| Variable | Default | Description |
|---|---|---|
| `JWT_SECRET` | (required) | HMAC-SHA256 secret for JWTs. Min 32 chars or server exits. |
| `HPC_SSH_USER` | `domeally` | SSH username for HPC clusters |
| `GEMINI_SSH_HOST` | `gemini-login2.coh.org` | Gemini login node hostname |
| `APOLLO_SSH_HOST` | `ppxhpcacc01.coh.org` | Apollo login node hostname |
| `DEFAULT_HPC` | `gemini` | Default cluster for new sessions |
| `DEFAULT_IDE` | `vscode` | Default IDE for new sessions |
| `DEFAULT_CPUS` | `2` | Default CPU count |
| `DEFAULT_MEM` | `40G` | Default memory allocation |
| `DEFAULT_TIME` | `12:00:00` | Default walltime |
| `DB_PATH` | `/data/app.db` | SQLite database path |
| `STATE_FILE` | `/data/state.json` | JSON state file (legacy; used with `ENABLE_STATE_PERSISTENCE`) |
| `ENABLE_STATE_PERSISTENCE` | `true` | Also write sessions to JSON file (SQLite is always used) |
| `STATUS_CACHE_TTL` | `1800000` | Cluster status cache TTL in ms (30 min default) |
| `LOG_LEVEL` | `info` | Winston log level |
| `DEBUG_COMPONENTS` | (empty) | Comma-separated debug namespaces (e.g., `ssh,state,tunnel,all`) |
| `SESSION_IDLE_TIMEOUT` | `0` | Minutes idle before auto-cancel (0 = disabled) |
| `SESSION_EXPIRY_DAYS` | `7` | JWT token expiry |
| `ADDITIONAL_PORTS` | `5500,3838` | Dev server ports checked by `/api/dev-servers` |
| `HPC_PROXY_LOCAL_PORT` | `9000` | Local port for hpc-proxy SSH tunnel |

### Clusters (`config/index.ts`)

```typescript
clusters: {
  gemini: {
    host: 'gemini-login2.coh.org',
    partition: 'compute',
    singularityBin: '/packages/easy-build/software/singularity/3.7.0/bin/singularity',
    bindPaths: '/packages,/scratch,/ref_genomes',
  },
  apollo: {
    host: 'ppxhpcacc01.coh.org',
    partition: 'fast,all',
    singularityBin: '/opt/singularity/3.7.0/bin/singularity',
    bindPaths: '/opt,/labs',
  }
}
```

### Bioconductor Releases (`config/index.ts`)

```typescript
releases: {
  '3.22': {
    name: 'Bioconductor 3.22',
    ides: ['vscode', 'rstudio', 'jupyter'],
    paths: {
      gemini: {
        singularityImage: '/packages/singularity/shared_cache/rbioc/rbiocverse_3.22.sif',
        rLibsSite: '/packages/.../rlibs/bioc-3.22',
        pythonEnv: '/packages/.../python/bioc-3.22',
      },
      apollo: {
        singularityImage: '/opt/singularity-images/rbioc/rbiocverse_3.22.sif',
        ...
      }
    }
  },
  '3.19': { ides: ['vscode', 'rstudio'], ... },
  '3.18': { ides: ['vscode', 'rstudio'], ... },
  '3.17': { ides: ['vscode', 'rstudio'], ... },
}
```

### GPU Configuration (Gemini only)

```typescript
gpuConfig: {
  gemini: {
    a100: { partition: 'gpu-a100', gres: 'gpu:A100:1', maxTime: '4-00:00:00', mem: '256G' },
    v100: { partition: 'gpu-v100', gres: 'gpu:V100:1', maxTime: '8-00:00:00', mem: '96G' },
  },
  apollo: null,  // No GPU support
}
```

## Database

SQLite (better-sqlite3, WAL mode) at `/data/app.db`.

| Table | Purpose |
|---|---|
| `users` | User accounts, encrypted SSH keys, `setup_complete` flag |
| `active_sessions` | Live SLURM sessions written through from `SessionManager` |
| `session_history` | Completed/archived sessions for analytics |
| `cluster_health` | Time-series health snapshots (30-min interval) |
| `cluster_cache` | Current cluster health state (last check result) |
| `partition_limits` | Dynamic SLURM partition info fetched from `sinfo` |
| `app_state` | Key-value store (currently: `activeSession`) |

Session keys (`<user>-<hpc>-<ide>`) are used consistently across `active_sessions`, `session_history`, and the in-memory `AppState.sessions` map.

## Security

### Input Validation

All user inputs are validated before use in shell commands:

- **CPUs**: Integer within partition limits (up to 128)
- **Memory**: Format `\d+[gGmM]` (e.g., "40G", "100M"); within partition limits
- **Time**: Format `HH:MM:SS` or `D-HH:MM:SS`; within partition limits
- **HPC name**: Whitelist (`gemini`, `apollo`)
- **GPU type**: Validated against `gpuConfig` for the cluster

### SSH Key Security

- Private keys encrypted at rest with AES-256-GCM using server-derived key (v3 format)
- Keys stored in `data/ssh-keys/<username>.key` with 600 permissions
- Per-user isolation: each SSH connection uses only the requesting user's key
- ControlMaster sockets at `/tmp/rbiocverse-ssh/<user>-<cluster>` (30-min persist)

### JWT Authentication

- HMAC-SHA256 signed, configurable expiry (default 7 days)
- Sliding session refresh: token auto-renewed when >50% expired (`X-Refreshed-Token` response header)
- Timing-safe comparison via `crypto.timingSafeEqual`

## Frontend (React UI)

React 19 SPA built with Vite 7 and TypeScript. No UI framework; custom CSS only.

**Provider hierarchy:**
```
ThemeProvider â†’ AuthProvider â†’ SessionStateProvider â†’ AppContent
```

**Key hooks:**
- `useClusterStatus` â€” polls `GET /api/cluster-status` every 2 seconds
- `useLaunch` â€” manages SSE `EventSource` for launch/connect flows
- `useCountdown` â€” client-side 1-second countdown for running job time
- `useApi` â€” central HTTP client with JWT header injection and auto-logout on 401

**SSE event types** (launch stream):
- `progress` â€” updates dual progress bars in `LoadingOverlay`
- `pending` â€” job queued; closes stream, shows pending session card
- `complete` â€” closes stream, navigates to IDE (`window.location.href`)
- `error` â€” shows error; SSH errors offer "Set up SSH Keys" shortcut

## Testing

```bash
# Unit + integration tests (471 tests)
npm test

# With coverage report
npm run test:coverage

# Type check (backend)
npm run typecheck

# Type check (frontend)
cd ui && npm run typecheck

# Frontend production build
cd ui && npm run build
```

## Logging

Winston with domain-specific log methods (`lib/logger.ts`):

```typescript
log.ssh('Executing command', { cluster: 'gemini' });
log.job('Submitted', { jobId: '12345' });
log.tunnel('Established', { port: 8000 });
log.api('POST /launch', { hpc: 'gemini' });
log.state('Session updated', { user, hpc, ide });
log.audit('Session started', { user, hpc, ide, jobId });  // Always logged
log.error('Failed', errorDetails(err));                   // With stack trace
```

Set `DEBUG_COMPONENTS=ssh,state,tunnel,cache,db,port-proxy,perf` for verbose output.
In production: logs at `/data/logs/manager.log` (5 MB max, 3 rotations).

## Help System

Built-in documentation with live cluster data. See [HELP_SYSTEM.md](HELP_SYSTEM.md) for details.

- **Template syntax**: `{{gemini.cpus.percent}}` renders live values
- **Ternary expressions**: `{{cluster.online ? "ğŸŸ¢" : "ğŸ”´"}}`
- **Widget embedding**: `:::widget ClusterHealth cluster="gemini":::`
- **Search**: Full-text search across all help sections

| File | Purpose |
|---|---|
| `routes/help.ts` | API + server-side template processing |
| `ui/src/components/HelpPanel.tsx` | Slide-out panel (uses ContentPanel) |
| `ui/src/components/ContentPanel.tsx` | Shared panel renderer (Help + Admin) |
| `ui/src/components/help-widgets/` | Embeddable live data widgets |
| `content/help/*.md` | Markdown content |
